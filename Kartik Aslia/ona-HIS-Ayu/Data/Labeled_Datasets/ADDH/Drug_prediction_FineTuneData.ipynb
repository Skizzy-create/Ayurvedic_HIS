{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "with open('C:\\\\Users\\\\LENOVO\\\\PycharmProjects\\\\Ayurvedic_HIS\\\\Kartik Aslia\\\\ona-HIS-Ayu\\\\Data\\\\Labeled_Datasets\\\\ADDH\\\\Drug prescription Dataset.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_data = []\n",
    "for row in data:\n",
    "    age = row[1]\n",
    "    disease = row[0]\n",
    "    severity = row[3]\n",
    "    drug = row[4]\n",
    "    formatted_string = f\"###user: my 'age' is {age} and I am suffering from {disease}, severity is '{severity}', ###assistant: you must have '{drug}' it will help you\"\n",
    "    converted_data.append(formatted_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data randomly\n",
    "random.shuffle(converted_data)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(0.8 * len(converted_data))\n",
    "\n",
    "# Split the data into train and eval sets\n",
    "train_data = converted_data[:split_index]\n",
    "eval_data = converted_data[split_index:]\n",
    "\n",
    "# Save train data to JSON file\n",
    "with open('train_data.json', 'w') as train_file:\n",
    "    for item in train_data:\n",
    "        json.dump(item, train_file)\n",
    "        train_file.write('\\n')\n",
    "\n",
    "# Save eval data to JSON file\n",
    "with open('eval_data.json', 'w') as eval_file:\n",
    "    for item in eval_data:\n",
    "        json.dump(item, eval_file)\n",
    "        eval_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_data = []\n",
    "for row in data:\n",
    "    age = row[1]\n",
    "    disease = row[0]\n",
    "    severity = row[3]\n",
    "    drug = row[4]\n",
    "    formatted_dict = {\n",
    "        \"user_age\": age,\n",
    "        \"user_disease\": disease,\n",
    "        \"disease_severity\": severity,\n",
    "        \"assistant_drug\": drug\n",
    "    }\n",
    "    converted_data.append(formatted_dict)\n",
    "\n",
    "# Shuffle the data randomly\n",
    "random.shuffle(converted_data)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(0.8 * len(converted_data))\n",
    "\n",
    "# Split the data into train and eval sets\n",
    "train_data = converted_data[:split_index]\n",
    "eval_data = converted_data[split_index:]\n",
    "\n",
    "# Save train data to JSON file\n",
    "with open('train_data.json', 'w') as train_file:\n",
    "    for item in train_data:\n",
    "        json.dump(item, train_file)\n",
    "        train_file.write('\\n')\n",
    "\n",
    "# Save eval data to JSON file\n",
    "with open('eval_data.json', 'w') as eval_file:\n",
    "    for item in eval_data:\n",
    "        json.dump(item, eval_file)\n",
    "        eval_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_data = []\n",
    "for row in data:\n",
    "    age = row[1]\n",
    "    disease = row[0]\n",
    "    severity = row[3]\n",
    "    drug = row[4]\n",
    "    formatted_dict = {\n",
    "        \"input\": f\"###User: Hi, my age is {age}, I have {disease}, its severity is {severity}\",\n",
    "        \"output\": f\"###Assistant: You must take {drug} to better your health\"\n",
    "    }\n",
    "    converted_data.append(formatted_dict)\n",
    "    \n",
    "    \n",
    "# Shuffle the data randomly\n",
    "random.shuffle(converted_data)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(0.8 * len(converted_data))\n",
    "\n",
    "# Split the data into train and eval sets\n",
    "train_data = converted_data[:split_index]\n",
    "eval_data = converted_data[split_index:]\n",
    "\n",
    "# Save train data to JSON file\n",
    "with open('train_data_new.json', 'w') as train_file:\n",
    "    for item in train_data:\n",
    "        json.dump(item, train_file)\n",
    "        train_file.write('\\n')\n",
    "\n",
    "# Save eval data to JSON file\n",
    "with open('eval_data_new.json', 'w') as eval_file:\n",
    "    for item in eval_data:\n",
    "        json.dump(item, eval_file)\n",
    "        eval_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(converted_data)\n",
    "df.to_csv('output.txt', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f0abaef2d2432882063cd975673dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2 named text expected length 258 but got length 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LENOVO\\PycharmProjects\\Ayurvedic_HIS\\Kartik Aslia\\ona-HIS-Ayu\\Data\\Labeled_Datasets\\ADDH\\Drug_prediction_FineTuneData.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/PycharmProjects/Ayurvedic_HIS/Kartik%20Aslia/ona-HIS-Ayu/Data/Labeled_Datasets/ADDH/Drug_prediction_FineTuneData.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: [text]}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/PycharmProjects/Ayurvedic_HIS/Kartik%20Aslia/ona-HIS-Ayu/Data/Labeled_Datasets/ADDH/Drug_prediction_FineTuneData.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Format the datasets using the formatting function\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/PycharmProjects/Ayurvedic_HIS/Kartik%20Aslia/ona-HIS-Ayu/Data/Labeled_Datasets/ADDH/Drug_prediction_FineTuneData.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mmap(formatting_func, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/PycharmProjects/Ayurvedic_HIS/Kartik%20Aslia/ona-HIS-Ayu/Data/Labeled_Datasets/ADDH/Drug_prediction_FineTuneData.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m eval_dataset \u001b[39m=\u001b[39m eval_dataset\u001b[39m.\u001b[39mmap(formatting_func, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[0;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\datasets\\arrow_dataset.py:3493\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3491\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_pandas(batch))\n\u001b[0;32m   3492\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3493\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[0;32m   3494\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3495\u001b[0m \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m _time \u001b[39m+\u001b[39m config\u001b[39m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\datasets\\arrow_writer.py:558\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    556\u001b[0m         inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[0;32m    557\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[1;32m--> 558\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_arrays(arrays, schema\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\pyarrow\\table.pxi:3879\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\pyarrow\\table.pxi:3159\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Column 2 named text expected length 258 but got length 1"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "# Load the train and evaluation datasets\n",
    "train_dataset = load_dataset('json', data_files='C:\\\\Users\\\\LENOVO\\\\PycharmProjects\\\\Ayurvedic_HIS\\\\Kartik Aslia\\\\ona-HIS-Ayu\\\\Data\\\\Labeled_Datasets\\\\ADDH\\\\eval_data_new.json', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='C:\\\\Users\\\\LENOVO\\\\PycharmProjects\\\\Ayurvedic_HIS\\\\Kartik Aslia\\\\ona-HIS-Ayu\\\\Data\\\\Labeled_Datasets\\\\ADDH\\\\eval_data_new.json', split='train')\n",
    "\n",
    "# Create a formatting function to structure training examples as prompts\n",
    "def formatting_func(example):\n",
    "    input_text = example.get('input', '')\n",
    "    output_text = example.get('output', '')\n",
    "    if not input_text or not output_text:\n",
    "        # Handle missing or empty input/output fields\n",
    "        return {'text': []}\n",
    "    text = f\"### Question: {input_text}\\n### Answer: {output_text}\"\n",
    "    return {'text': [text]}\n",
    "\n",
    "\n",
    "\n",
    "# Format the datasets using the formatting function\n",
    "train_dataset = train_dataset.map(formatting_func, batched=True)\n",
    "eval_dataset = eval_dataset.map(formatting_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439432de9be745a8aca8cd26f720420e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\.conda\\envs\\cuda\\lib\\site-packages\\transformers\\utils\\hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "name7b = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "auth_token = \"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\"\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name7b, cache_dir=\"D:\\\\LLM\")\n",
    "\n",
    "# Load model directly\n",
    "model = AutoModelForCausalLM.from_pretrained(name7b, cache_dir=\"D:\\\\LLM\", use_auth_token=auth_token, torch_dtype=torch.float16, rope_scaling={\"type\":\"dynamic\",\"factor\":2}, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"fine_tuned_model\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
